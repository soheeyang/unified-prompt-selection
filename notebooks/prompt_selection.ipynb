{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/soheeyang/unified-prompt-selection/blob/main/notebooks/prompt_selection.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/>\n",
    "</a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    # Setting up an environment for Google Colab.\n",
    "\n",
    "    import google.colab, sys\n",
    "\n",
    "    install_script = \"\"\"#!/usr/bin/bash\n",
    "\n",
    "    !(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "    cd /content && rm -rf /content/unified-prompt-selection\n",
    "    git clone https://github.com/soheeyang/unified-prompt-selection.git unified-prompt-selection > install.log 2>&1\n",
    "    pip install -r /content/unified-prompt-selection/requirements.txt >> install.log 2>&1\n",
    "    pip install --upgrade google-cloud-storage >> install.log 2>&1\"\"\"\n",
    "\n",
    "    with open(\"/content/install.sh\", \"w\") as f:\n",
    "        f.write(install_script)\n",
    "\n",
    "    os.system(\"bash /content/install.sh\")\n",
    "    os.chdir(\"/content/unified-prompt-selection\")\n",
    "    sys.path.append(\"/content/unified-prompt-selection\")\n",
    "except ModuleNotFoundError as _:\n",
    "    os.chdir(\"/content/unified-prompt-selection\")\n",
    "    os.system(\"pip install -r requirements.txt > install.log 2>&1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Selection\n",
    "\n",
    "By running `run_prompt_selection.py`, you can extract $p(y|x,t)$ from LLMs and select a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can obtain four different results depending on whether calibration is applied:\n",
    "\n",
    " * **X**: without applying any calibration\n",
    "\n",
    " * **A**: applying calibration only for **A**nswer selection\n",
    "\n",
    " * **P**: applying calibration only for **P**rompt selection\n",
    "\n",
    " * **PA**: applying calibration for both **P**rompt selection and **A**nswer selection.\n",
    "\n",
    "Notes:\n",
    "- When applying prompt selection to a dataset without ground truth labels, prompt selection is possible, but evaluation results cannot be verified. **Before applying prompt selection to a dataset, check whether ground truth labels are available or not** or **check whether the evaluation results for datasets without ground truth labels are based on the Target of the prompt selection results.**\n",
    "\n",
    "Running the command as shown above will execute Prompt Selection according to the predefined default arguments.\n",
    "\n",
    "We used [hydra](https://hydra.cc/docs/intro/) to manage complex configurations. You can check the configurations in [`./conf`](./conf/), and besides specifying arguments on the command line, you can modify the arguments by editing the [`./conf/config.yaml`](./conf/config.yaml) file.\n",
    "\n",
    "You can also execute various combinations by adding `-m` or `--multirun` as follows:\n",
    "\n",
    "```bash\n",
    "python run_prompt_selection.py -m \\\n",
    "    method=MI,MI_G,MI_L,MI_GL,GE,GE_M,LE,MDL,MDL_M,ZLP,ZPM,ZMV,PPL,PPL_L \\\n",
    "    calibration=cbm-softmax,cbm-mean,cc-softmax,cc-mean,pmi-softmax,pmi-mean \\\n",
    "    decoder=opt-1.3b,opt-2.7b,opt-6.7b,opt-30b,opt-66b,gpt-neo-1.3b,gpt-neo-2.7b,gpt-j-6b,gpt2-xl,bloom-3b \\\n",
    "    dataset=sst2,ag_news,cb,imdb,newspop,rte,sst5,tweet_emotion,tweet_irony,piqa,copa,hellaswag,story_cloze \\\n",
    "    prompt=base_prompts,v12_prompts,v2_prompts,fewshot_prompt \\\n",
    "    first_token=false,true \\\n",
    "    sum_log_prob=false,true \\\n",
    "    fewshot=null,'1,2,4' \\\n",
    "    filter=false,true \\\n",
    "    unbalance=false,true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py -m \\\n",
    "    method=MI,MI_G,MI_L,MI_GL,GE,GE_M,LE,MDL,MDL_M,ZLP,ZPM,ZMV,PPL,PPL_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability-based Prompt Selection Method\n",
    "\n",
    "<center><img src=\"../images/ps_methods.png\" width=\"100%\" height=\"100%\"></center>\n",
    "\n",
    "The following probability-based prompt selection methods are available: 'MI', 'GE', 'LE', 'MDL', 'ZLP', 'ZPM', 'ZMV', and 'PPL'.\n",
    "\n",
    "To use a specific prompt selection method, pass the desired method to `method`. You can find detailed descriptions of each method in section 2.2 Existing Approaches of the [paper](https://arxiv.org/pdf/2305.14877.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants created by Prompt Selection Methods\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "<img src=\"../images/variants.png\" width=\"55%\" height=\"55%\">\n",
    "</p>\n",
    "\n",
    "The following methods are variants that modify the score calculation formula of existing Probability-based prompt selection methods: 'MI_G', 'MI_L,' 'MI_GL', 'GE_M', 'MDL_M', and 'PPL_L'.\n",
    "\n",
    "You can check the arguments specific to these probability-based prompt selection methods in the [`./conf/method`](./conf/method/) directory.\n",
    "\n",
    "If a method name is followed by '_L', it means that [`select_for_each_x`](./conf/method/MI_L.yaml) is set to 'True', and instance-wise prompt selection is performed. The methods that support instance-wise prompt selection are 'MDL', 'MI', and 'PPL'.\n",
    "\n",
    "If a method name is followed by '_G', it means that [`one_hot`](./conf/method/MI_G.yaml) is set to 'True', and one-hot $p(y|x,t)$ is used for GE calculation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-selection",
   "language": "python",
   "name": "prompt-selection"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
