{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/soheeyang/unified-prompt-selection/blob/main/unified_prompt_selection.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/>\n",
    "</a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    # Setting up an environment for Google Colab.\n",
    "\n",
    "    import google.colab, sys, torch\n",
    "\n",
    "    install_script = \"\"\"#!/usr/bin/bash\n",
    "\n",
    "    !(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "    cd /content && rm -rf /content/unified-prompt-selection\n",
    "    git clone https://github.com/soheeyang/unified-prompt-selection.git unified-prompt-selection > install.log 2>&1\n",
    "    pip install -r /content/unified-prompt-selection/requirements.txt >> install.log 2>&1\n",
    "    pip install --upgrade google-cloud-storage >> install.log 2>&1\"\"\"\n",
    "\n",
    "    with open(\"/content/install.sh\", \"w\") as f:\n",
    "        f.write(install_script)\n",
    "\n",
    "    os.system(\"bash /content/install.sh\")\n",
    "    os.chdir(\"/content/unified-prompt-selection\")\n",
    "    sys.path.append(\"/content/unified-prompt-selection\")\n",
    "    if not torch.cuda.is_available():\n",
    "        raise Exception(\"Change runtime type to include a GPU.\")\n",
    "except ModuleNotFoundError as _:\n",
    "    os.system(\"pip install -r requirements.txt > install.log 2>&1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis\n",
    "\n",
    "Official code for the paper [Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis](https://arxiv.org/abs/2305.14877) accepted at TACL 2024.\n",
    "\n",
    "You can read the summary of our paper in [this Twitter Thread](https://twitter.com/soheeyang_/status/1661339578240737287).\n",
    "\n",
    "Use the following to cite our paper:\n",
    "\n",
    "```jsx\n",
    "@article{yang2024improving,\n",
    "  title={Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis},\n",
    "  author={Yang, Sohee and Kim, Jonghyeon and Jang, Joel and Ye, Seonghyeon and Lee, Hyunji and Seo, Minjoon},\n",
    "  journal={TACL},\n",
    "  year={2024}\n",
    "}\n",
    "```\n",
    "\n",
    "This repository provides a set of tools for easy utilization and unified evaluation of different probability-based prompt selection methods.\n",
    "\n",
    "If you're interested in reproducing the experimental results mentioned in the paper, please refer to the [Reproduction](#reproduction) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "1. Extraction of the language model's output probability necessary for prompt selection score calculation.\n",
    "2. Probability-based prompt selection methods.\n",
    "3. Calibration methods.\n",
    "4. Task-wise prompt selection and Instance-wise prompt selection.\n",
    "5. Custom prompt addition that conforms to the template format of [promptsource](https://github.com/bigscience-workshop/promptsource)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "<center><img src=\"images/Overview.png\" width=\"100%\" height=\"100%\"></center>\n",
    "\n",
    "LLMs predict the essential $p(y|x,t)$ through inference with given prompt candidates and datasets to calculate prompt selection scores. In the subsequent prompt selection process, the extracted $p(y|x,t)$ is loaded to calculate prompt selection scores. Based on these scores, a prompt is chosen, and the selection result is returned. OTR(One-Token Response) Converter is used when calculating $p(y|x,t)$ by utilizing only the first token logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "You can quickly explore the core functionalities by referring to the following notebook files.\n",
    "\n",
    "- **Prompt Selection**\n",
    "\n",
    "    [![Colab PS](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/soheeyang/unified-prompt-selection/blob/main/notebooks/prompt_selection.ipynb)\n",
    "\n",
    "- **Adding Custom Prompt Selection Method**\n",
    "\n",
    "    [![Colab PS](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/soheeyang/unified-prompt-selection/blob/main/notebooks/add_custom_prompt_selection_method.ipynb)\n",
    "\n",
    "- **Adding Custom Prompt**\n",
    "\n",
    "    [![Colab PS](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/soheeyang/unified-prompt-selection/blob/main/notebooks/add_custom_prompt.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Inference & Prompt Selection\n",
    "\n",
    "By running `run_prompt_selection.py`, you can extract $p(y|x,t)$ and select a prompt.\n",
    "\n",
    "The extracted $p(y|x,t)$ through LLM inference is stored in the './extraction/results' directory. When running `run_prompt_selection.py` with the same configuration, it utilizes the previously saved extraction results without additional inference. For detailed information about inference, please refer to the $p(y|x,t)$ [Extraction](#pyxt-extraction) section.\n",
    "\n",
    "```bash\n",
    "python run_prompt_selection.py\n",
    "```\n",
    "Running the command as shown above will execute Prompt Selection according to the predefined default arguments.\n",
    "\n",
    "You can also execute various combinations by adding `-m` or `--multirun` as follows:\n",
    "\n",
    "```bash\n",
    "python run_prompt_selection.py -m \\\n",
    "    method=MI,MI_G,MI_L,MI_GL,GE,GE_M,LE,MDL,MDL_M,ZLP,ZPM,ZMV,PPL,PPL_L \\\n",
    "    calibration=cbm-softmax,cbm-mean,cc-softmax,cc-mean,pmi-softmax,pmi-mean \\\n",
    "    decoder=opt-1.3b,opt-2.7b,opt-6.7b,opt-30b,opt-66b,gpt-neo-1.3b,gpt-neo-2.7b,gpt-j-6b,gpt2-xl,bloom-3b \\\n",
    "    dataset=sst2,ag_news,cb,imdb,newspop,rte,sst5,tweet_emotion,tweet_irony,piqa,copa,hellaswag,story_cloze \\\n",
    "    prompt=base_prompts,v12_prompts,v2_prompts,fewshot_prompt \\\n",
    "    first_token=false,true \\\n",
    "    sum_log_prob=false,true \\\n",
    "    fewshot=null,'1,2,4' \\\n",
    "    filter=false,true \\\n",
    "    unbalance=false,true\n",
    "```\n",
    "\n",
    "We used [hydra](https://hydra.cc/docs/intro/) to manage complex configurations. You can check the configurations in [`./conf`](./conf/), and besides specifying arguments on the command line, you can modify the arguments by editing the [`./conf/config.yaml`](./conf/config.yaml) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can obtain four different results depending on whether calibration is applied:\n",
    "\n",
    " * **X**: without applying any calibration\n",
    "\n",
    " * **A**: applying calibration only for **A**nswer selection\n",
    "\n",
    " * **P**: applying calibration only for **P**rompt selection\n",
    "\n",
    " * **PA**: applying calibration for both **P**rompt selection and **A**nswer selection.\n",
    "\n",
    "The prompt selection result is saved in the [`‘./results/dataset=\"\"__decoder=\"\"__prompt=\"\"__first_token=\"\"__sum_log_prob=\"\"__num_samples=\"\"__seed=\"\"__fewshot=\"\"__do_eval=\"\"/method=\"\"__using_all_tokens=\"\"__one_hot=\"\"__select_for_each_x=\"\"__filter=\"\"__unbalance=\"\".json’`](./results/) file. The format of the result file is as follows:\n",
    "\n",
    "<details>\n",
    "<summary>Prompt selection result format</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```json\n",
    "    {\n",
    "        \"X\": {\n",
    "            \"method\": \"...\",\n",
    "            \"cali_type\": \"...\",\n",
    "            \"cali_norm_type\": \"...\",\n",
    "            \"accuracy\": \"...\",\n",
    "            \"macro_f1\": \"...\",\n",
    "            \"model\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"task\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"token\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"instance\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"prompt\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"prediction\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"target\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ]\n",
    "        },\n",
    "    \t\"A\": {\n",
    "            \"method\": \"...\",\n",
    "            \"cali_type\": \"...\",\n",
    "            \"cali_norm_type\": \"...\",\n",
    "            \"accuracy\": \"...\",\n",
    "            \"macro_f1\": \"...\",\n",
    "            \"model\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"task\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"token\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"instance\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"prompt\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"prediction\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"target\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ]\n",
    "        },\n",
    "        \"P\": {\n",
    "            \"method\": \"...\",\n",
    "            \"cali_type\": \"...\",\n",
    "            \"cali_norm_type\": \"...\",\n",
    "            \"accuracy\": \"...\",\n",
    "            \"macro_f1\": \"...\",\n",
    "            \"model\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"task\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"token\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"instance\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"prompt\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"prediction\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"target\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ]\n",
    "        },\n",
    "        \"PA\": {\n",
    "            \"method\": \"...\",\n",
    "            \"cali_type\": \"...\",\n",
    "            \"cali_norm_type\": \"...\",\n",
    "            \"accuracy\": \"...\",\n",
    "            \"macro_f1\": \"...\",\n",
    "            \"model\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"task\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"token\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"instance\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"prompt\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"prediction\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ],\n",
    "            \"target\": [\n",
    "              \"...\",\n",
    "              \"...\"\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "Notes:\n",
    "- When applying prompt selection to a dataset without ground truth labels, prompt selection is possible, but evaluation results cannot be verified. **Before applying prompt selection to a dataset, check whether ground truth labels are available or not** or **check whether the evaluation results for datasets without ground truth labels are based on the Target of the prompt selection results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability-based Prompt Selection Method\n",
    "\n",
    "<center><img src=\"images/ps_methods.png\" width=\"100%\" height=\"100%\"></center>\n",
    "\n",
    "The following probability-based prompt selection methods are available: 'MI', 'GE', 'LE', 'MDL', 'ZLP', 'ZPM', 'ZMV', and 'PPL'.\n",
    "\n",
    "To use a specific prompt selection method, pass the desired method to `method`. You can find detailed descriptions of each method in section 2.2 Existing Approaches of the [paper](https://arxiv.org/pdf/2305.14877.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants created by Prompt Selection Methods\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "<img src=\"images/variants.png\" width=\"55%\" height=\"55%\">\n",
    "</p>\n",
    "\n",
    "The following methods are variants that modify the score calculation formula of existing Probability-based prompt selection methods: 'MI_G', 'MI_L,' 'MI_GL', 'GE_M', 'MDL_M', and 'PPL_L'.\n",
    "\n",
    "You can check the arguments specific to these probability-based prompt selection methods in the [`./conf/method`](./conf/method/) directory.\n",
    "\n",
    "If a method name is followed by '_L', it means that [`select_for_each_x`](./conf/method/MI_L.yaml) is set to 'True', and instance-wise prompt selection is performed. The methods that support instance-wise prompt selection are 'MDL', 'MI', and 'PPL'.\n",
    "\n",
    "If a method name is followed by '_G', it means that [`one_hot`](./conf/method/MI_G.yaml) is set to 'True', and one-hot $p(y|x,t)$ is used for GE calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py -m \\\n",
    "    method=MI,MI_G,MI_L,MI_GL,GE,GE_M,LE,MDL,MDL_M,ZLP,ZPM,ZMV,PPL,PPL_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Custom Prompt Selection Method\n",
    "\n",
    "If you want to add a new prompt selection method, refer to the [./method/methods.py](./method/methods.py) module. Check other methods in this module, and create a new method according to the types and dimensions of input and output values.\n",
    "\n",
    "||Task-wise Method|Instance-wise Method|\n",
    "|:---------:|:-------:|:------------:|\n",
    "|Input Variable|`template_prob`|`tensor_dict_prob`|\n",
    "|Input Variable Type|torch.Tensor|torch.Tensor|\n",
    "|Input Variable Size|[X, Y]|[T, X, Y]|\n",
    "|Output Value Type|torch.Tensor|Tuple[List[float], List[int]]|\n",
    "|Output Value Size|[]\\(Scalar\\)|([X], [X])|\n",
    "\n",
    "- **T** : The number of prompts\n",
    "- **X** : The number of instances\n",
    "- **Y** : The number of answer choices\n",
    "\n",
    "The Instance-wise Method has two lists as output values. The first list should contain the results of calculated instance-wise prompt selection scores using the added method, and the second list should contain the indices of the selected prompts for each instance.\n",
    "\n",
    "The newly added method is utilized in the `get_*_wise_ps_scores` function in the [./method/score.py](./method/score.py) module.\n",
    "\n",
    "```python\n",
    "methodFuncMap = {\n",
    "    'MI': get_mi_g if one_hot else get_mi,\n",
    "    'GE': get_ge if one_hot else get_ge_m,\n",
    "    'LE': get_le,\n",
    "    'MDL': get_le,\n",
    "    'PPL': get_ppl,\n",
    "    'ZLP': get_zlp,\n",
    "    'ZPM': get_zpm,\n",
    "    'ZMV': get_zmv,\n",
    "}\n",
    "```\n",
    "\n",
    "When adding a method, follow the format of the method in the methodFuncMap inside the `get_*_wise_ps_result` function. Additionally, add a yaml file representing the new method to [`./conf/method`](./conf/method/) directory. Refer to the yaml file format of other methods in `./conf/method`\n",
    "\n",
    "The following example demonstrates the process of adding a new method by adjusting the weights of GE based on the default configuration, creating template_prob and tensor_dict_prob variables, and examining their shapes and types. The newly added method modifies GE weights, and this process is illustrated step by step.\n",
    "\n",
    "The default configuration focuses on the 'glue-sst2-validation' dataset, consisting of 872 instances(X) and 2 answer choices(Y). There are 100 prompts(T) based on base_prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Base Configuration\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "get_config = \"\"\"\n",
    "import hydra\n",
    "import pickle\n",
    "\n",
    "@hydra.main(version_base=None, config_path=\"conf\", config_name=\"config\")\n",
    "def main(cfg):\n",
    "    with open(\"cfg.pkl\", \"wb\") as f:\n",
    "        pickle.dump(cfg, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "with open(\"get_config.py\", \"w\") as f:\n",
    "    f.write(get_config)\n",
    "os.system(\"python get_config.py\")\n",
    "os.remove(\"get_config.py\")\n",
    "\n",
    "with open(\"cfg.pkl\", \"rb\") as f:\n",
    "    cfg = pickle.load(f)\n",
    "os.remove(\"cfg.pkl\")\n",
    "\n",
    "# Check `template_prob` and `tensor_dict_prob`\n",
    "from method.postprocessor import PostProcessor\n",
    "\n",
    "post_processor = PostProcessor(cfg)\n",
    "tensor_dict, targets = post_processor.get_tensor_dict_and_targets()\n",
    "template_prob, tensor_dict_prob = tensor_dict['prob'][0], tensor_dict['prob']\n",
    "\n",
    "input_var_info = f'''\n",
    "{'-'*40}\n",
    "template_prob Dimension: {template_prob.dim()}\n",
    "template_prob Size: [{template_prob.size(0)}, {template_prob.size(1)}]\n",
    "\n",
    "tensor_dict_prob Dimension: {tensor_dict_prob.dim()}\n",
    "tensor_dict_prob Size: [{tensor_dict_prob.size(0)}, {tensor_dict_prob.size(1)}, {tensor_dict_prob.size(2)}]\n",
    "{'-'*40}\n",
    "'''\n",
    "\n",
    "print(input_var_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some prompt selection method functions from [./method/methods.py](./method/methods.py). Let's create new methods, MI with triple GE weight (3GE_MI) and MI_L with triple GE weight (3GE_MI_L), considering the output format of existing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def safe_log(prob: Tensor) -> Tensor:\n",
    "    return torch.log(prob + 1e-7)\n",
    "\n",
    "def get_entropy(prob: Tensor, sum_axis: int, keepdims: bool = False) -> Tensor:\n",
    "    return -(prob * safe_log(prob)).sum(axis=sum_axis, keepdims=keepdims)\n",
    "\n",
    "def get_le(template_prob: Tensor) -> Tensor:\n",
    "    return get_entropy(template_prob, sum_axis=-1).mean()\n",
    "\n",
    "def get_ge_m(template_prob: Tensor, X_axis: int = 0, Y_axis: int = -1, keepdims: bool = False) -> Tensor:\n",
    "    d_prob = template_prob.mean(axis=X_axis, keepdims=keepdims)\n",
    "    return get_entropy(d_prob, sum_axis=Y_axis, keepdims=keepdims)\n",
    "\n",
    "def get_mi(template_prob: Tensor) -> Tensor:\n",
    "    ge_m = get_ge_m(template_prob)\n",
    "    mdl_m = get_le(template_prob)\n",
    "    return (ge_m - mdl_m)\n",
    "\n",
    "def get_mi_l(tensor_dict_prob: Tensor) -> Tuple[List[float], List[int]]:\n",
    "    tensor_dict_prob = tensor_dict_prob.transpose(0, 1)  # [X, T, Y]\n",
    "    ge_m = get_ge_m(tensor_dict_prob, X_axis=0, Y_axis=-1)\n",
    "\n",
    "    X = tensor_dict_prob.size(0)\n",
    "    mi_als, selected_prompt_indices = [], []\n",
    "    for i in range(X):\n",
    "        instance_prob = tensor_dict_prob[i]  # [T, Y]\n",
    "        mdl = get_entropy(instance_prob, -1)\n",
    "        mi_al = ge_m - mdl\n",
    "        mi_als.append(mi_al.max().item())\n",
    "        selected_prompt_indices.append(mi_al.argmax().item())\n",
    "    return mi_als, selected_prompt_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new methods are created by simply utilizing the existing functions as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3ge_mi(template_prob: Tensor) -> Tensor:\n",
    "    ge_m = 3 * get_ge_m(template_prob)\n",
    "    mdl_m = get_le(template_prob)\n",
    "    return (ge_m - mdl_m)\n",
    "\n",
    "def get_3ge_mi_l(tensor_dict_prob: Tensor) -> Tuple[List[float], List[int]]:\n",
    "    tensor_dict_prob = tensor_dict_prob.transpose(0, 1)  # [X, T, Y]\n",
    "    ge_m = 3 * get_ge_m(tensor_dict_prob, X_axis=0, Y_axis=-1)\n",
    "\n",
    "    X = tensor_dict_prob.size(0)\n",
    "    mi_als, selected_prompt_indices = [], []\n",
    "    for i in range(X):\n",
    "        instance_prob = tensor_dict_prob[i]  # [T, Y]\n",
    "        mdl = get_entropy(instance_prob, -1)\n",
    "        mi_al = ge_m - mdl\n",
    "        mi_als.append(mi_al.max().item())\n",
    "        selected_prompt_indices.append(mi_al.argmax().item())\n",
    "    return mi_als, selected_prompt_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the output of the newly added methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_of_task_wise_method = get_3ge_mi(template_prob)\n",
    "output_of_instance_wise_method = get_3ge_mi_l(tensor_dict_prob)\n",
    "\n",
    "output_value_info = f'''\n",
    "{'-'*200}\n",
    "Task-wise Method Type: {type(output_of_task_wise_method)}\n",
    "Task-wise Method Output: {output_of_task_wise_method}\n",
    "\n",
    "\n",
    "Instance-wise Method Type: ( {type(output_of_instance_wise_method)} [ {type(output_of_instance_wise_method[0])}, {type(output_of_instance_wise_method[1])} ] )\n",
    "Instance-wise Method Size: [ {len(output_of_instance_wise_method[0])}, {len(output_of_instance_wise_method[1])} ]\n",
    "\n",
    "Instance-wise Method Output, Index == 0: \\n{output_of_instance_wise_method[0]}\n",
    "\n",
    "Instance-wise Method Output, Index == 1: \\n{output_of_instance_wise_method[1]}\n",
    "{'-'*200}\n",
    "'''\n",
    "\n",
    "print(output_value_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, these added methods must be included in the methodFuncMap of the get_task_wise_ps_result and get_instance_wise_ps_result functions in [./method/score.py](./method/score.py).\n",
    "\n",
    "```python\n",
    "# methodFuncMap in `get_task_wise_ps_result`\n",
    "methodFuncMap = {\n",
    "    'MI': get_mi_g if one_hot else get_mi,\n",
    "    'GE': get_ge if one_hot else get_ge_m,\n",
    "    'LE': get_le,\n",
    "    'MDL': get_le,\n",
    "    'PPL': get_ppl,\n",
    "    'ZLP': get_zlp,\n",
    "    'ZPM': get_zpm,\n",
    "    'ZMV': get_zmv,\n",
    "    '3GE_MI': get_3ge_mi\n",
    "}\n",
    "\n",
    "# methodFuncMap in `get_instance_wise_ps_result`\n",
    "methodFuncMap = {\n",
    "    'PPL': get_i_ppl,\n",
    "    'MDL': get_mdl,\n",
    "    'MI': get_mi_gl if one_hot else get_mi_l,\n",
    "    '3GE_MI': get_3ge_mi_l\n",
    "}\n",
    "```\n",
    "\n",
    "Lastly, include the YAML files for the new methods in [./conf/method](./conf/method/), and you can use them directly through the run_prompt_selection.py script.\n",
    "\n",
    "---\n",
    "\n",
    "```yaml\n",
    "## './conf/method/3GE_MI.yaml'\n",
    "\n",
    "# Pass the prompt selection method to use as an argument.\n",
    "method: '3GE_MI'\n",
    "\n",
    "# If passed, the GE is computed with one hot P(y|x,t).\n",
    "one_hot: false\n",
    "\n",
    "# If passed, a prompt is selected for each x.\n",
    "select_for_each_x: false\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```yaml\n",
    "## './conf/method/3GE_MI_L.yaml'\n",
    "\n",
    "# Pass the prompt selection method to use as an argument.\n",
    "method: '3GE_MI'\n",
    "\n",
    "# If passed, the GE is computed with one hot P(y|x,t).\n",
    "one_hot: false\n",
    "\n",
    "# If passed, a prompt is selected for each x.\n",
    "select_for_each_x: true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "python run_prompt_selection \\\n",
    "    method=3GE_MI \\\n",
    "    select_for_each_x=false,true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Method\n",
    "\n",
    "Three calibration methods are available, allowing you to observe the changes in prompt selection results before and after applying each calibration.\n",
    "\n",
    "You can check the arguments related to calibration in the [`./conf/calibration`](./conf/calibration/) directory.\n",
    "\n",
    "You can choose one of the following calibration methods by specifying 'cbm', 'cc', or 'pmi' to [`cali_type`](./conf/calibration/cbm-softmax.yaml). The default value is 'cbm'.\n",
    "\n",
    "- **cbm**: Calibration By Marginalization, an enhanced calibration method proposed in this work that demonstrates effective results in prompt selection compared to existing methods.\n",
    "- **cc**: Contextual Calibration, a calibration method proposed in [Calibrate before use: Improving Few-Shot performance of language models](https://arxiv.org/pdf/2102.09690.pdf).\n",
    "- **pmi**: Domain Conditional PMI, a calibration method proposed in [Surface form competition: Why the highest probability answer isn’t always right](https://arxiv.org/pdf/2104.08315.pdf).\n",
    "\n",
    "You can specify the normalization criteria for $\\tilde{q}(y|x,t)$ using [`cali_norm_type`](./conf/calibration/cbm-softmax.yaml). The options are 'softmax' or 'mean', and the default value is 'softmax'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py -m \\\n",
    "    calibration=cbm-mean,cbm-softmax,cc-mean,cc-softmax,pmi-mean,pmi-softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Dataset\n",
    "Dynamic datasets are datasets where answer choices are not fixed and vary for each instance. The experiment includes COPA, PIQA, StoryCloze, and Hellaswag as dynamic datasets. With such datasets, you can adjust the label distribution differently and observe the changes in prompt selection results based on the label distribution.\n",
    "\n",
    "If you set [`unbalance`](./conf/config.yaml) to 'True', you can adjust the label distribution of COPA, PIQA, StoryCloze, and Hellaswag to be unbalanced at a ratio of 100:0 and observe the prompt selection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py -m \\\n",
    "    dataset=copa \\\n",
    "    unbalance=false,true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "In [Zero-Label Prompt Selection](https://arxiv.org/abs/2211.04668), a low-quality prompt filtering algorithm using k-means clustering and $p(y|x,t)$ was proposed. By setting [`filter`](./conf/config.yaml) to 'True', prompts can be filtered using the zero-label prompt selection (ZPS) filtering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py -m \\\n",
    "    filter=true,false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $p(y|x,t)$ Extraction\n",
    "\n",
    "To perform probability-based prompt selection, you need to extract the output probability of the language model, denoted as $p(y|x,t)$, where $x,t$ represents the instantiated prompt.\n",
    "\n",
    "If desired, it is also possible to perform LLM inference excluding prompt selection and extract $p(y|x,t)$ using the `run_inference.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_inference.py \\\n",
    "    decoder=opt-2.7b \\\n",
    "    dataset=sst2 \\\n",
    "    prompt=base_prompts \\\n",
    "    first_token=false \\\n",
    "    sum_log_prob=false \\\n",
    "    num_samples=1000 \\\n",
    "    seed=42 \\\n",
    "    fewshot=null \\\n",
    "    do_eval=true \\\n",
    "    mixed_precision=no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments that affect extraction can be found in [`./conf/config.yaml`](./conf/config.yaml).\n",
    "\n",
    "To calculate $p(y|x,t)$ using only the first token of $y$, set the [`first_token`](./conf/config.yaml) to 'True'. If set to 'False', all tokens of $y$ are used.\n",
    "\n",
    "When calculating $p(y|x,t)$ using all tokens over $y$, you have the option to calculate either the mean log-probability or the sum of log-probability for all tokens over $y$. To extract the sum of log-probability for all tokens over $y$, set [`sum_log_prob`](./conf/config.yaml) to 'True'.\n",
    "\n",
    "To adjust the dataset sample size for prompt selection, use the [`num_samples`](./conf/config.yaml).\n",
    "\n",
    "For reproducibility, you can set the random seed by using the [`seed`](./conf/config.yaml).\n",
    "\n",
    "If you want to select one of the fewshot prompts with a different number and order of examples for a single prompt template, you can use [`fewshot`](./conf/config.yaml).\n",
    "<details>\n",
    "<summary>Detailed Explanation of the `fewshot`</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "- The values that can be passed to [`fewshot`](./conf/config.yaml) are '1', '2', '4', '1,2', '1,4', '2,4', '1,2,4'. Each number represents the number of randomly selected examples from the training dataset for the fewshot prompt.\n",
    "\n",
    "- Passing '1' saves eight outputs of fewshot prompts with a single example, and then one of the eight prompts is selected. Passing '2' saves 20 outputs, and passing '4' saves 72 outputs. If multiple values are passed by concatenating them with ',', 100 (8+20+72) outputs are saved, and one of the 100 fewshot prompts, which is expected to work best, is selected.\n",
    "</div>\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "Through the [`do_eval`](./conf/config.yaml), you can choose whether to record evaluation results ('Accuracy', 'Macro F1') for each prompt during the inference stage for extraction. If set to 'False,' prompt selection can still be performed even if there is no ground truth label in the dataset. However, you will only be able to verify what the final selected prompt is and its predictions.\n",
    "\n",
    "Notes:\n",
    "- The results of $p(y|x,t)$ extraction are saved under the [`./extraction/results/dataset=\"\"__decoder=\"\"__prompt=\"\"__first_token=\"\"__sum_log_prob=\"\"__num_samples=\"\"__seed=\"\"__fewshot=\"\"__do_eval=\"\"`](./extraction/results/) directory. \n",
    "- If results have already been extracted with the same arguments (i.e., the directory with the same name exists under [`./extraction/results`](./extraction/results/)), the existing results will be provided, and additional extraction will not be performed.\n",
    "- If a directory with the same name exists but the names of the selected prompt templates have changed in the settings, additional extraction will only be carried out for prompts with names that were not previously extracted.\n",
    "- $p(y|x,t)$ extraction results from each prompt will be saved as JSON files under the directory. The output file has the following format:\n",
    "\n",
    "<details>\n",
    "<summary>Extraction output format</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```json\n",
    "    {\n",
    "        \"dataset_name\": \"...\",\n",
    "        \"dataset_config_name\": \"...\",\n",
    "        \"template_name\": \"...\",\n",
    "        \"evaluation\": null,\n",
    "        \"raw\": {\n",
    "            \"inputs\": [\n",
    "              \"...\",\n",
    "              \"...\",\n",
    "              \"...\",\n",
    "            ],\n",
    "            \"predictions\": [\n",
    "              \"...\",\n",
    "              \"...\",\n",
    "              \"...\",\n",
    "            ],\n",
    "            \"targets\": [\n",
    "              \"...\",\n",
    "              \"...\",\n",
    "              \"...\",\n",
    "            ],\n",
    "            \"accuracy\": [\n",
    "              \"...\",\n",
    "              \"...\",\n",
    "              \"...\",\n",
    "            ],\n",
    "            \"f1\": null,\n",
    "            \"log_prob\": [\n",
    "              [\n",
    "                \"...\",\n",
    "                \"...\"\n",
    "              ],\n",
    "              [\n",
    "                \"...\",\n",
    "                \"...\"\n",
    "              ],\n",
    "              [\n",
    "                \"...\",\n",
    "                \"...\"\n",
    "              ],\n",
    "            ],\n",
    "            \"empty_log\": [\n",
    "              [\n",
    "                \"...\",\n",
    "                \"...\"\n",
    "              ]\n",
    "            ],\n",
    "            \"na_log\": [\n",
    "              [\n",
    "                \"...\",\n",
    "                \"...\"\n",
    "              ]\n",
    "            ],\n",
    "            \"mask_log\": [\n",
    "              [\n",
    "                \"...\",\n",
    "                \"...\"\n",
    "              ]\n",
    "            ],\n",
    "            \"P(x,t)\": [\n",
    "              \"...\",\n",
    "              \"...\",\n",
    "              \"...\",\n",
    "            ],\n",
    "            \"P(t)\": [\n",
    "              \"...\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "If you want to specify a particular model, you can pass the desired model name to the decoder. The models used in the experiment can be found in [`./conf/decoder`](./conf/decoder/).\n",
    "\n",
    "You can also add new models by adding .yaml files under [`./conf/decoder`](./conf/decoder/). You can refer to existing files to configure the arguments.\n",
    "\n",
    "Pass the desired model name to [`model_name_or_path`](./conf/decoder/opt-2.7b.yaml). Adding models is limited to the decoder models supported by the Hugging Face [`transformers`](https://huggingface.co/models) library.\n",
    "\n",
    "❗ Verify the `pad_token_id` of the tokenizer to be used and pass the verified `pad_token_id` to [`ignore_index`](./conf/decoder/opt-2.7b.yaml).\n",
    "\n",
    "To apply model parallelism, use the [`parallelize`](./conf/decoder/opt-2.7b.yaml).\n",
    "\n",
    "You can adjust the batch size by using the [`per_device_eval_batch_size`](./conf/decoder/opt-2.7b.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "If you want to specify a particular dataset, you can pass the desired dataset name to `dataset`. The datasets used in the experiment can be found in [`./conf/dataset`](./conf/dataset/).\n",
    "\n",
    "You can also add new datasets by adding `.yaml` files under [`./conf/dataset`](./conf/dataset/). You can refer to existing files to configure the arguments. To manage and utilize prompts, we rely on [promptsource](https://github.com/bigscience-workshop/promptsource). Therefore, adding new datasets is limited to classification [datasets existing in promptsource](./extraction/promptsource/templates/).\n",
    "\n",
    "[`dataset_name`](./conf/dataset/story_cloze.yaml) corresponds to the `path` argument of the [`load_dataset`](https://huggingface.co/docs/datasets/v2.13.0/en/package_reference/loading_methods#datasets.load_dataset) method in the Hugging Face [`datasets`](https://huggingface.co/datasets) library.\n",
    "\n",
    "[`dataset_config_name`](./conf/dataset/story_cloze.yaml) corresponds to the `name` argument.\n",
    "\n",
    "[`split`](./conf/dataset/story_cloze.yaml) corresponds to the `split` argument.\n",
    "\n",
    "[`DATASET_KWARGS`](./conf/dataset/story_cloze.yaml) are automatically determined based on the configured [`dataset_name`](./conf/dataset/story_cloze.yaml), [`dataset_config_name`](./conf/dataset/story_cloze.yaml), and [`split`](./conf/dataset/story_cloze.yaml).\n",
    "\n",
    "[`DATASET_INFO`](./conf/dataset/story_cloze.yaml) is required for OTR (=One Token Response, [`first_token`](./conf/config.yaml)=True).\n",
    "- [`num_classes`](./conf/dataset/story_cloze.yaml) should be set to the number of label categories.\n",
    "- [`label`](./conf/dataset/story_cloze.yaml) should be set to the column name containing ground truth labels for the dataset. If it doesn't exist, you can set it to null. In such cases, don't forget to set the `do_eval` option to 'False' as well.\n",
    "- [`is_dynamic`](./conf/dataset/story_cloze.yaml) should be set to True if the dataset you want to add is a dynamic task; otherwise, set it to False. For an explanation of dynamic tasks, refer to [Dynamic Dataset](#dynamic-dataset).\n",
    "- [`choices`](./conf/dataset/story_cloze.yaml) should be set to values corresponding to the [answer_choices](./extraction/promptsource/templates/story_cloze/2016/templates.yaml) associated with the dataset's prompt templates.\n",
    "\n",
    "[`TEMPLATE_INFO`](./conf/dataset/story_cloze.yaml) is required for [Custom Prompt Addition](#custom-prompt-addition). Set [`text_format`](./conf/dataset/story_cloze.yaml) to the column name containing text in the appropriate format and [`jinja_suffix`](./conf/dataset/story_cloze.yaml) to the column name containing ground truth labels in the format specified by [jinja](./extraction/promptsource/templates/story_cloze/2016/templates.yaml).\n",
    "\n",
    "Note:\n",
    "\n",
    "- If you want to apply a dataset that is not present in promptsource, you will need a [`templates.yaml`](./extraction/promptsource/templates/story_cloze/2016/templates.yaml) file that adheres to the format used by promptsource. We recommend creating this file by referring to examples in the promptsource documentation and [`templates.yaml`](./extraction/promptsource/templates/story_cloze/2016/templates.yaml) files for other datasets. Also, please note that if there are no ground truth labels, you can create the [jinja](./extraction/promptsource/templates/story_cloze/2016/templates.yaml) section without adding [`jinja_suffix`](./conf/dataset/story_cloze.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "\n",
    "You can find the list of prompts used in the experiment in the `.yaml` files under [`conf/prompt`](./conf/prompt/).\n",
    "\n",
    "The prompts you want to use can be configured through `prompt`. Create a `.yaml` file under [`conf/prompt`](./conf/prompt/) and pass the name of the created file to prompt.\n",
    "\n",
    "[`prompt_config_name`](./conf/prompt/base_prompts.yaml) affects the directory name where the results of $p(y|x,t)$ extraction are stored (`'prompt=&{prompt_config_name}'`).\n",
    "\n",
    "By entering the [name](./extraction/promptsource/templates/story_cloze/2016/templates.yaml)s of prompt templates as a list in [`template_names`](./conf/prompt/base_prompts.yaml), you can select prompts from those templates.\n",
    "\n",
    "If you want to add new prompts, refer to [Custom Prompt Addition](#custom-prompt-addition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Custom Prompt\n",
    "\n",
    "To manage and utilize prompts, we rely on [promptsource](https://github.com/bigscience-workshop/promptsource). For more information on adding prompts, refer to promptsource.\n",
    "\n",
    "You can add a prompt by running add_prompt.py.\n",
    "\n",
    "Here is an example of adding a prompt for the 'ag_news' dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python add_prompt.py dataset=ag_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction\n",
    "\n",
    "When running `python add_prompt.py dataset=ag_news`, follow the instructions below:\n",
    "\n",
    "```\n",
    "Instruction example:\n",
    "\"{{ text }}\" is about\n",
    "\n",
    "Enter the instruction for the new prompt, using the example format provided above:\n",
    "```\n",
    "\n",
    "The placeholder '{{ ... }}' represents the location where the instance of the dataset will be inserted. The ... refers to the column name that contains the instances for each dataset.\n",
    "\n",
    "```\n",
    "Instruction example:\n",
    "\"{{ text }}\" is about\n",
    "\n",
    "Enter the instruction for the new prompt, using the example format provided above:\n",
    "{{ text }} this is test_add_prompt.\n",
    "\n",
    "Please verify that the entered instruction is correct:\n",
    "{{ text }} this is test_add_prompt.\n",
    "\n",
    "If the instruction is correct, press 'y'; otherwise, enter another key.\n",
    "If you choose another key, you will be able to edit the instruction: y\n",
    "```\n",
    "\n",
    "Follow the provided example and input the desired instruction accordingly.\n",
    "\n",
    "Notes:\n",
    "- <details>\n",
    "  <summary>The instruction must include '{{ ... }}'.</summary>\n",
    "  <div markdown=\"1\">\n",
    "\n",
    "  ```\n",
    "  Instruction example:\n",
    "  \"{{ text }}\" is about\n",
    "\n",
    "  Enter the instruction for the new prompt, using the example format provided\n",
    "  above:\n",
    "  this is test_add_prompt.\n",
    "\n",
    "  The instruction is missing '{{ text }}'. Please enter the instruction again.\n",
    "  ```\n",
    "\n",
    "  </div>\n",
    "  </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Choices\n",
    "\n",
    "```\n",
    "Answer_choices example:\n",
    "politics ||| sports ||| business ||| science\n",
    "\n",
    "Enter the answer_choices for the new prompt, using the example format provided above:\n",
    "```\n",
    "\n",
    "After entering the instruction, input the answer_choices.\n",
    "\n",
    "```\n",
    "Answer_choices example:\n",
    "politics ||| sports ||| business ||| science\n",
    "\n",
    "Enter the answer_choices for the new prompt, using the example format provided above:\n",
    "politics ||| sports ||| business ||| science\n",
    "\n",
    "Please verify that the entered answer_choices are correct:\n",
    "politics ||| sports ||| business ||| science\n",
    "\n",
    "If the answer_choices are correct, press 'y'; otherwise, enter another key.\n",
    "If you choose another key, you will be able to edit the answer_choices: y\n",
    "```\n",
    "\n",
    "Enter the desired answer_choices according to the provided example.\n",
    "\n",
    "Notes:\n",
    "- <details>\n",
    "  <summary>If you omit \" ||| \", you will need to enter the answer_choices again.</summary>\n",
    "  <div markdown=\"1\">\n",
    "\n",
    "  ```\n",
    "  Answer_choices example:\n",
    "  politics ||| sports ||| business ||| science\n",
    "\n",
    "  Enter the answer_choices for the new prompt, using the example format provided above:\n",
    "  politics sports business science\n",
    "\n",
    "  Each answer_choice must be entered separated by \" ||| \". Please enter the answer_choices again.\n",
    "\n",
    "  ```\n",
    "\n",
    "  </div>\n",
    "  </details>\n",
    "\n",
    "- <details>\n",
    "  <summary>If the number of answer_choices is different from the number of labels, you will need to enter the answer_choices again.</summary>\n",
    "  <div markdown=\"1\">\n",
    "\n",
    "  ```\n",
    "  Answer_choices example:\n",
    "  politics ||| sports ||| business ||| science\n",
    "\n",
    "  Enter the answer_choices for the new prompt, using the example format provided above:\n",
    "  politics ||| sports\n",
    "\n",
    "  The number of answer_choices must be 4. Please enter the answer_choices again.\n",
    "  ```\n",
    "\n",
    "  </div>\n",
    "  </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Name\n",
    "\n",
    "\n",
    "Prompt name is used to identify and utilize the added prompt. After entering the answer_choices, input the prompt name to identify the added prompt.\n",
    "\n",
    "```\n",
    "Enter the prompt name: test_add_prompt\n",
    "\n",
    "Please verify that the entered prompt name is correct:\n",
    "test_add_prompt\n",
    "\n",
    "If the prompt name is correct, press 'y'; otherwise, enter another key.\n",
    "If you choose another key, you will be able to edit the prompt name: y\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- <details>\n",
    "  <summary>The prompt name cannot be duplicated.</summary>\n",
    "  <div markdown=\"1\">\n",
    "\n",
    "  ```\n",
    "  Enter prompt name: prompt_00\n",
    "\n",
    "  A prompt with the same name already exists.\n",
    "  ```\n",
    "\n",
    "  </div>\n",
    "  </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Custom Prompt\n",
    "\n",
    "Added prompts are saved in the following files, depending on the [`dataset_name`](./conf/dataset/ag_news.yaml) and [`dataset_config_name`](./conf/dataset/ag_news.yaml):\n",
    "\n",
    "`./extraction/promptsource/templates/{dataset_name/dataset_config_name}/templates.yaml` \n",
    "\n",
    "You can check the example prompt that was added in the following file:\n",
    "\n",
    "[`./extraction/promptsource/templates/ag_news/templates.yaml`](./extraction/promptsource/templates/ag_news/templates.yaml)\n",
    "\n",
    "\n",
    "Here is an example of how to use the added prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example from the datasets ag_news\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "example = dataset[1]\n",
    "\n",
    "# Load prompts for this dataset\n",
    "from extraction.promptsource.templates import DatasetTemplates\n",
    "ag_news_prompts = DatasetTemplates('ag_news')\n",
    "\n",
    "# Select a prompt by its name\n",
    "prompt = ag_news_prompts[\"test_add_prompt\"]\n",
    "# prompt = ag_news_prompts[\"Your Prompt Name\"]\n",
    "\n",
    "# Apply the prompt to the example\n",
    "result = prompt.apply(example)\n",
    "print(\"INPUT: \", result[0])\n",
    "print(\"TARGET: \", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to load the ag_news dataset, load the prompts for the dataset, select the prompt named \"test_add_prompt,\" and apply it to an example from the dataset. The result shows the input with the applied prompt and the corresponding target label.\n",
    "\n",
    "## Reproduction\n",
    "\n",
    "```\n",
    "❗ To download the extracted p(y|x,t) output for the experiment, approximately 50GB of free space is required. \n",
    "   If you don't have enough space, you can directly download the Prompt Selection Score result.\n",
    "```\n",
    "\n",
    "If you want to reproduce the entire set of experimental results, including results for all datasets and models, please follow the instructions below.\n",
    "\n",
    "### Preparing $p(y|x,t)$\n",
    "To reproduce the results, you need to extract the required $p(y|x,t)$ for prompt selection. The $p(y|x,t)$ used in the experiment can be downloaded using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./reproduction/download_experimental_results.py --result inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- The prompts used in the experiment can be found in the [`./extraction/promptsource/templates`](./extraction/promptsource/templates/) directory. The prompts used in the experiment are specified in the `templates.yaml` file located in the directory of the dataset you want to reproduce.\n",
    "\n",
    "### Prompt Selection Score Result\n",
    "\n",
    "After preparing the $p(y|x,t)$, you can use the following command to download all prompt selection results in the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./reproduction/download_experimental_results.py --result prompt_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets, models, and prompt selection method results used in the experiment will be saved in the [`./reproduction/ps_results`](./reproduction/ps_results/) directory.\n",
    "\n",
    "### Visualizing the Result\n",
    "\n",
    "Once you have prepared [`./reproduction/ps_results`](./reproduction/ps_results/), you can refer to [`figures_ver2_tacl.ipynb`](./reproduction/figures_ver2_tacl.ipynb) and [`figures_ver1_arxiv.ipynb`](./reproduction/figures_ver1_arxiv.ipynb) to recreate the figures presented in the paper.\n",
    "\n",
    "---\n",
    "### Reproduce by Running Commands Directly\n",
    "Extracting $p(y|x,t)$ for all prompts, datasets, and models used in the experiment requires a significant amount of resources and time. However, if you want to reproduce the results directly, you can use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py -m \\\n",
    "    method=MI,MI_G,MI_L,MI_GL,GE,GE_M,LE,MDL,MDL_M,ZLP,ZPM,ZMV,PPL,PPL_L \\\n",
    "    calibration=cbm-softmax,cbm-mean,cc-softmax,cc-mean,pmi-softmax,pmi-mean \\\n",
    "    decoder=opt-1.3b,opt-2.7b,opt-6.7b,opt-30b,opt-66b,gpt-neo-1.3b,gpt-neo-2.7b,gpt-j-6b,gpt2-xl,bloom-3b \\\n",
    "    dataset=sst2,ag_news,cb,imdb,newspop,rte,sst5,tweet_emotion,tweet_irony \\\n",
    "    prompt=base_prompts \\\n",
    "    sum_log_prob=false \\\n",
    "    filter=false,true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py -m \\\n",
    "    method=MI,MI_G,MI_L,MI_GL,GE,GE_M,LE,MDL,MDL_M,ZLP,ZPM,ZMV,PPL,PPL_L \\\n",
    "    calibration=cbm-softmax,cbm-mean,cc-softmax,cc-mean,pmi-softmax,pmi-mean \\\n",
    "    decoder=opt-2.7b \\\n",
    "    dataset=sst2,ag_news,cb,imdb,newspop,rte,sst5,tweet_emotion,tweet_irony \\\n",
    "    prompt=v1_prompts,v12_prompts \\\n",
    "    sum_log_prob=false \\\n",
    "    filter=false,true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py -m \\\n",
    "    method=MI,MI_G,MI_L,MI_GL,GE,GE_M,LE,MDL,MDL_M,ZLP,ZPM,ZMV,PPL,PPL_L \\\n",
    "    calibration=cbm-softmax,cbm-mean,cc-softmax,cc-mean,pmi-softmax,pmi-mean \\\n",
    "    decoder=opt-2.7b \\\n",
    "    dataset=sst2,ag_news,cb,imdb,newspop,rte,sst5,tweet_emotion,tweet_irony \\\n",
    "    prompt=fewshot_prompt \\\n",
    "    fewshot='1,2,4' \\\n",
    "    sum_log_prob=false \\\n",
    "    filter=false,true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py -m \\\n",
    "    method=MI,MI_G,MI_L,MI_GL,GE,GE_M,LE,MDL,MDL_M,ZLP,ZPM,ZMV,PPL,PPL_L \\\n",
    "    calibration=cbm-softmax,cbm-mean,cc-softmax,cc-mean,pmi-softmax,pmi-mean \\\n",
    "    decoder=opt-1.3b,opt-2.7b,opt-6.7b,opt-30b,opt-66b,gpt-neo-1.3b,gpt-neo-2.7b,gpt-j-6b,gpt2-xl,bloom-3b \\\n",
    "    dataset=piqa,copa,hellaswag,story_cloze \\\n",
    "    prompt=base_prompts \\\n",
    "    sum_log_prob=true \\\n",
    "    filter=false,true \\\n",
    "    unbalance=false,true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py -m \\\n",
    "    method=MI,MI_G,MI_L,MI_GL,GE,GE_M,LE,MDL,MDL_M,ZLP,ZPM,ZMV,PPL,PPL_L \\\n",
    "    calibration=cbm-softmax,cbm-mean,cc-softmax,cc-mean,pmi-softmax,pmi-mean \\\n",
    "    decoder=opt-2.7b \\\n",
    "    dataset=piqa,copa,hellaswag,story_cloze \\\n",
    "    prompt=v1_prompts,v2_prompts \\\n",
    "    sum_log_prob=true \\\n",
    "    filter=false,true \\\n",
    "    unbalance=false,true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_prompt_selection.py -m \\\n",
    "    method=MI,MI_G,MI_L,MI_GL,GE,GE_M,LE,MDL,MDL_M,ZLP,ZPM,ZMV,PPL,PPL_L \\\n",
    "    calibration=cbm-softmax,cbm-mean,cc-softmax,cc-mean,pmi-softmax,pmi-mean \\\n",
    "    decoder=opt-2.7b \\\n",
    "    dataset=piqa,copa,hellaswag,story_cloze \\\n",
    "    prompt=fewshot_prompt \\\n",
    "    fewshot='1,2,4' \\\n",
    "    sum_log_prob=true \\\n",
    "    filter=false,true \\\n",
    "    unbalance=false,true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-selection",
   "language": "python",
   "name": "prompt-selection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
